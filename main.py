import asyncio
import aiohttp
import re
import yaml
import os
import base64
from urllib.parse import quote
from urllib.parse import urlparse
from tqdm import tqdm
from loguru import logger

# å…¨å±€é…ç½®
RE_URL = r"https?://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]"
CHECK_NODE_URL_STR = "https://{}/sub?target={}&url={}&insert=false&config=config%2FACL4SSR.ini"
CHECK_URL_LIST = ['api.dler.io', 'sub.xeton.dev', 'sub.id9.cc', 'sub.maoxiongnet.com']

# -------------------------------
# é…ç½®æ–‡ä»¶æ“ä½œ
# -------------------------------
def load_yaml_config(path_yaml):
    """è¯»å– YAML é…ç½®æ–‡ä»¶ï¼Œå¦‚æ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿”å›é»˜è®¤ç»“æ„"""
    if os.path.exists(path_yaml):
        with open(path_yaml, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    else:
        config = {
            "æœºåœºè®¢é˜…": [],
            "clashè®¢é˜…": [],
            "v2è®¢é˜…": [],
            "å¼€å¿ƒç©è€": [],
            "tgchannel": []
        }
    return config

def save_yaml_config(config, path_yaml):
    """ä¿å­˜é…ç½®åˆ° YAML æ–‡ä»¶"""
    with open(path_yaml, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, allow_unicode=True)

def get_config_channels(config_file='config.yaml'):
    """
    ä»é…ç½®æ–‡ä»¶ä¸­è·å– Telegram é¢‘é“é“¾æ¥ï¼Œ
    å°†ç±»ä¼¼ https://t.me/univstar è½¬æ¢ä¸º https://t.me/s/univstar æ ¼å¼
    """
    config = load_yaml_config(config_file)
    tgchannels = config.get('tgchannel', [])
    new_list = []
    for url in tgchannels:
        parts = url.strip().split('/')
        if parts:
            channel_id = parts[-1]
            new_list.append(f'https://t.me/s/{channel_id}')
    return new_list

# -------------------------------
# å¼‚æ­¥ HTTP è¯·æ±‚è¾…åŠ©å‡½æ•°
# -------------------------------
async def fetch_content(url, session, method='GET', headers=None, timeout=15):
    """è·å–æŒ‡å®š URL çš„æ–‡æœ¬å†…å®¹"""
    try:
        async with session.request(method, url, headers=headers, timeout=timeout) as response:
            if response.status == 200:
                text = await response.text()
                return text
            else:
                logger.warning(f"URL {url} è¿”å›çŠ¶æ€ {response.status}")
                return None
    except Exception as e:
        logger.error(f"è¯·æ±‚ {url} å¼‚å¸¸: {e}")
        return None

# -------------------------------
# é¢‘é“æŠ“å–åŠè®¢é˜…æ£€æŸ¥
# -------------------------------
async def get_channel_urls(channel_url, session):
    """ä» Telegram é¢‘é“é¡µé¢æŠ“å–æ‰€æœ‰è®¢é˜…é“¾æ¥ï¼Œå¹¶è¿‡æ»¤æ— å…³é“¾æ¥"""
    content = await fetch_content(channel_url, session)
    if content:
        # æå–æ‰€æœ‰ URLï¼Œå¹¶æ’é™¤åŒ…å«â€œ//t.me/â€æˆ–â€œcdn-telegram.orgâ€çš„é“¾æ¥
        all_urls = re.findall(RE_URL, content)
        filtered = [u for u in all_urls if "//t.me/" not in u and "cdn-telegram.org" not in u]
        logger.info(f"ä» {channel_url} æå– {len(filtered)} ä¸ªé“¾æ¥")
        return filtered
    else:
        logger.warning(f"æ— æ³•è·å– {channel_url} çš„å†…å®¹")
        return []

async def sub_check(url, session):
    """
    æ”¹è¿›çš„è®¢é˜…æ£€æŸ¥å‡½æ•°ï¼š
      - åˆ¤æ–­å“åº”å¤´ä¸­çš„ subscription-userinfo ç”¨äºæœºåœºè®¢é˜…
      - åˆ¤æ–­å†…å®¹ä¸­æ˜¯å¦åŒ…å« 'proxies:' åˆ¤å®š clash è®¢é˜…
      - å°è¯• base64 è§£ç åˆ¤æ–­ v2 è®¢é˜…ï¼ˆè¯†åˆ« ss://ã€ssr://ã€vmess://ã€trojan://ã€vless://ï¼‰
      - å¢åŠ é‡è¯•æœºåˆ¶å’Œæ›´å¥½çš„é”™è¯¯å¤„ç†
    è¿”å›ä¸€ä¸ªå­—å…¸ï¼š{"url": ..., "type": ..., "info": ...}
    """
    headers = {
        'User-Agent': 'ClashforWindows/0.18.1',
        'Accept': '*/*',
        'Accept-Encoding': 'gzip, deflate'
    }
    
    # é‡è¯•æœºåˆ¶
    for attempt in range(2):
        try:
            async with session.get(url, headers=headers, timeout=12) as response:
                if response.status == 200:
                    text = await response.text()
                    
                    # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©ºæˆ–è¿‡çŸ­
                    if not text or len(text.strip()) < 10:
                        logger.debug(f"è®¢é˜… {url} å†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­")
                        return None
                    
                    result = {"url": url, "type": None, "info": None}
                    
                    # åˆ¤æ–­æœºåœºè®¢é˜…ï¼ˆæ£€æŸ¥æµé‡ä¿¡æ¯ï¼‰
                    sub_info = response.headers.get('subscription-userinfo')
                    if sub_info:
                        nums = re.findall(r'\d+', sub_info)
                        if len(nums) >= 3:
                            upload, download, total = map(int, nums[:3])
                            if total > 0:  # ç¡®ä¿æ€»æµé‡å¤§äº0
                                unused = (total - upload - download) / (1024 ** 3)
                                if unused > 0:
                                    result["type"] = "æœºåœºè®¢é˜…"
                                    result["info"] = f"å¯ç”¨æµé‡: {round(unused, 2)} GB"
                                    return result
                    
                    # åˆ¤æ–­ clash è®¢é˜… - æ›´ä¸¥æ ¼çš„æ£€æŸ¥
                    if "proxies:" in text and ("name:" in text or "server:" in text):
                        proxy_count = text.count("- name:")
                        if proxy_count > 0:
                            result["type"] = "clashè®¢é˜…"
                            result["info"] = f"åŒ…å« {proxy_count} ä¸ªèŠ‚ç‚¹"
                            return result
                    
                    # åˆ¤æ–­ v2 è®¢é˜…ï¼Œé€šè¿‡ base64 è§£ç æ£€æµ‹
                    try:
                        # æ£€æŸ¥æ˜¯å¦å¯èƒ½æ˜¯base64ç¼–ç ï¼ˆæ›´å®½æ¾çš„æ£€æŸ¥ï¼‰
                        text_clean = text.strip().replace('\n', '').replace('\r', '')
                        if len(text_clean) > 20:
                            try:
                                # å°è¯•è§£ç 
                                decoded = base64.b64decode(text_clean).decode('utf-8', errors='ignore')
                                protocols = ['ss://', 'ssr://', 'vmess://', 'trojan://', 'vless://']
                                found_protocols = [proto for proto in protocols if proto in decoded]
                                
                                if found_protocols:
                                    node_count = sum(decoded.count(proto) for proto in found_protocols)
                                    if node_count > 0:
                                        result["type"] = "v2è®¢é˜…"
                                        result["info"] = f"åŒ…å« {node_count} ä¸ªèŠ‚ç‚¹ (base64)"
                                        logger.debug(f"è®¢é˜… {url} è¯†åˆ«ä¸ºbase64ç¼–ç çš„v2è®¢é˜…ï¼ŒåŒ…å« {node_count} ä¸ªèŠ‚ç‚¹")
                                        return result
                                else:
                                    # æ£€æŸ¥è§£ç åæ˜¯å¦åŒ…å«é…ç½®å…³é”®å­—
                                    config_keywords = ['server', 'port', 'password', 'method', 'host', 'path']
                                    if any(keyword in decoded.lower() for keyword in config_keywords):
                                        lines = [line.strip() for line in decoded.split('\n') if line.strip()]
                                        if len(lines) > 0:
                                            result["type"] = "v2è®¢é˜…"
                                            result["info"] = f"åŒ…å« {len(lines)} è¡Œé…ç½® (base64)"
                                            logger.debug(f"è®¢é˜… {url} è¯†åˆ«ä¸ºbase64ç¼–ç çš„é…ç½®æ–‡ä»¶")
                                            return result
                            except Exception:
                                # base64è§£ç å¤±è´¥ï¼Œç»§ç»­å…¶ä»–æ£€æŸ¥
                                pass
                    except Exception as e:
                        logger.debug(f"è®¢é˜… {url} base64æ£€æµ‹å¼‚å¸¸: {e}")
                        pass
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯åŸå§‹æ ¼å¼çš„v2è®¢é˜…
                    protocols = ['ss://', 'ssr://', 'vmess://', 'trojan://', 'vless://']
                    found_protocols = [proto for proto in protocols if proto in text]
                    if found_protocols:
                        node_count = sum(text.count(proto) for proto in found_protocols)
                        if node_count > 0:
                            result["type"] = "v2è®¢é˜…"
                            result["info"] = f"åŒ…å« {node_count} ä¸ªèŠ‚ç‚¹ (åŸå§‹)"
                            logger.debug(f"è®¢é˜… {url} è¯†åˆ«ä¸ºåŸå§‹æ ¼å¼çš„v2è®¢é˜…")
                            return result
                    
                    
                    # å¦‚æœå†…å®¹çœ‹èµ·æ¥åƒé…ç½®ä½†ä¸åŒ¹é…å·²çŸ¥æ ¼å¼ï¼Œè®°å½•è°ƒè¯•ä¿¡æ¯
                    if len(text) > 100:
                        # æ˜¾ç¤ºå†…å®¹çš„å‰100ä¸ªå­—ç¬¦ç”¨äºè°ƒè¯•
                        preview = text[:100].replace('\n', '\\n').replace('\r', '\\r')
                        logger.info(f"âš ï¸  è®¢é˜… {url} å†…å®¹ä¸åŒ¹é…å·²çŸ¥æ ¼å¼")
                        logger.info(f"   é•¿åº¦: {len(text)} å­—ç¬¦")
                        logger.info(f"   é¢„è§ˆ: {preview}...")
                        
                        # æ£€æŸ¥æ˜¯å¦å¯èƒ½æ˜¯å…¶ä»–æ ¼å¼
                        if 'http' in text.lower() or 'server' in text.lower():
                            logger.info(f"   å¯èƒ½åŒ…å«æœåŠ¡å™¨é…ç½®ï¼Œä½†æ ¼å¼æœªè¯†åˆ«")
                    
                    return None
                    
                elif response.status in [403, 404, 410, 500]:
                    # è¿™äº›çŠ¶æ€ç é€šå¸¸è¡¨ç¤ºæ°¸ä¹…å¤±è´¥
                    logger.debug(f"è®¢é˜…æ£€æŸ¥ {url} è¿”å›çŠ¶æ€ {response.status}")
                    return None
                else:
                    logger.warning(f"è®¢é˜…æ£€æŸ¥ {url} è¿”å›çŠ¶æ€ {response.status}")
                    if attempt == 0:  # ç¬¬ä¸€æ¬¡å¤±è´¥ï¼Œé‡è¯•
                        await asyncio.sleep(1)
                        continue
                    return None
                    
        except asyncio.TimeoutError:
            logger.debug(f"è®¢é˜…æ£€æŸ¥ {url} è¶…æ—¶ï¼Œå°è¯• {attempt + 1}/2")
            if attempt == 0:
                await asyncio.sleep(1)
                continue
        except Exception as e:
            logger.debug(f"è®¢é˜…æ£€æŸ¥ {url} å¼‚å¸¸: {e}ï¼Œå°è¯• {attempt + 1}/2")
            if attempt == 0:
                await asyncio.sleep(1)
                continue
    
    return None

# -------------------------------
# èŠ‚ç‚¹æœ‰æ•ˆæ€§æ£€æµ‹ï¼ˆæ ¹æ®å¤šä¸ªæ£€æµ‹å…¥å£ï¼‰
# -------------------------------
async def url_check_valid(url, target, session):
    """
    æ”¹è¿›çš„èŠ‚ç‚¹æœ‰æ•ˆæ€§æ£€æµ‹ï¼š
    é€šè¿‡éå†å¤šä¸ªæ£€æµ‹å…¥å£æ£€æŸ¥è®¢é˜…èŠ‚ç‚¹æœ‰æ•ˆæ€§ï¼Œ
    ä¸ä»…æ£€æŸ¥çŠ¶æ€ç ï¼Œè¿˜éªŒè¯è¿”å›å†…å®¹çš„æœ‰æ•ˆæ€§ã€‚
    """
    encoded_url = quote(url, safe='')
    
    for check_base in CHECK_URL_LIST:
        check_url = CHECK_NODE_URL_STR.format(check_base, target, encoded_url)
        try:
            async with session.get(check_url, timeout=20) as resp:
                if resp.status == 200:
                    content = await resp.text()
                    
                    # æ£€æŸ¥è¿”å›å†…å®¹æ˜¯å¦æœ‰æ•ˆ
                    if not content or len(content.strip()) < 50:
                        logger.debug(f"èŠ‚ç‚¹æ£€æµ‹ {url} åœ¨ {check_base} è¿”å›å†…å®¹è¿‡çŸ­")
                        continue
                    
                    # æ ¹æ®ç›®æ ‡ç±»å‹éªŒè¯å†…å®¹
                    if target == "clash":
                        if "proxies:" in content and ("name:" in content or "server:" in content):
                            proxy_count = content.count("- name:")
                            if proxy_count > 0:
                                logger.debug(f"èŠ‚ç‚¹æ£€æµ‹ {url} åœ¨ {check_base} æˆåŠŸï¼ŒåŒ…å« {proxy_count} ä¸ªèŠ‚ç‚¹")
                                return url
                    elif target == "loon":
                        # Loonæ ¼å¼é€šå¸¸åŒ…å«[Proxy]æ®µè½
                        if "[Proxy]" in content or "=" in content:
                            logger.debug(f"èŠ‚ç‚¹æ£€æµ‹ {url} åœ¨ {check_base} æˆåŠŸ (Loonæ ¼å¼)")
                            return url
                    elif target == "v2ray":
                        # V2Rayæ ¼å¼å¯èƒ½æ˜¯JSONæˆ–å…¶ä»–æ ¼å¼
                        if len(content.strip()) > 100:  # åŸºæœ¬é•¿åº¦æ£€æŸ¥
                            logger.debug(f"èŠ‚ç‚¹æ£€æµ‹ {url} åœ¨ {check_base} æˆåŠŸ (V2Rayæ ¼å¼)")
                            return url
                    else:
                        # å…¶ä»–æ ¼å¼ï¼ŒåŸºæœ¬é•¿åº¦æ£€æŸ¥
                        if len(content.strip()) > 100:
                            logger.debug(f"èŠ‚ç‚¹æ£€æµ‹ {url} åœ¨ {check_base} æˆåŠŸ")
                            return url
                    
                    logger.debug(f"èŠ‚ç‚¹æ£€æµ‹ {url} åœ¨ {check_base} å†…å®¹æ ¼å¼ä¸åŒ¹é…")
                else:
                    logger.debug(f"èŠ‚ç‚¹æ£€æµ‹ {url} åœ¨ {check_base} è¿”å›çŠ¶æ€ {resp.status}")
                    
        except asyncio.TimeoutError:
            logger.debug(f"èŠ‚ç‚¹æ£€æµ‹ {url} åœ¨ {check_base} è¶…æ—¶")
            continue
        except Exception as e:
            logger.debug(f"èŠ‚ç‚¹æ£€æµ‹ {url} åœ¨ {check_base} å¼‚å¸¸: {e}")
            continue
    
    logger.debug(f"èŠ‚ç‚¹æ£€æµ‹ {url} åœ¨æ‰€æœ‰æ£€æµ‹ç‚¹éƒ½å¤±è´¥")
    return None

# -------------------------------
# ä¸»æµç¨‹ï¼šæ›´æ–°è®¢é˜…ä¸åˆå¹¶
# -------------------------------
async def update_today_sub(session):
    """
    ä» Telegram é¢‘é“è·å–æœ€æ–°è®¢é˜…é“¾æ¥ï¼Œ
    è¿”å›ä¸€ä¸ªå»é‡åçš„ URL åˆ—è¡¨
    """
    tg_channels = get_config_channels('config.yaml')
    all_urls = []
    for channel in tg_channels:
        urls = await get_channel_urls(channel, session)
        all_urls.extend(urls)
    return list(set(all_urls))

async def check_subscriptions(urls):
    """
    å¼‚æ­¥æ£€æŸ¥æ‰€æœ‰è®¢é˜…é“¾æ¥çš„æœ‰æ•ˆæ€§ï¼Œ
    è¿”å›æ£€æŸ¥ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœä¸ºå­—å…¸ {url, type, info}
    """
    if not urls:
        return []
    
    results = []
    # åˆ›å»ºè¿æ¥å™¨ï¼Œé™åˆ¶å¹¶å‘è¿æ¥æ•°
    connector = aiohttp.TCPConnector(
        limit=100,
        limit_per_host=20,
        ttl_dns_cache=300,
        use_dns_cache=True,
    )
    
    timeout = aiohttp.ClientTimeout(total=30, connect=10)
    
    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(50)
        
        async def check_single(url):
            async with semaphore:
                return await sub_check(url, session)
        
        tasks = [check_single(url) for url in urls]
        for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="è®¢é˜…ç­›é€‰"):
            res = await coro
            if res:
                results.append(res)
    return results

async def check_nodes(urls, target, session):
    """
    å¼‚æ­¥æ£€æŸ¥æ¯ä¸ªè®¢é˜…èŠ‚ç‚¹çš„æœ‰æ•ˆæ€§ï¼Œ
    è¿”å›æ£€æµ‹æœ‰æ•ˆçš„èŠ‚ç‚¹ URL åˆ—è¡¨
    """
    if not urls:
        return []
    
    valid_urls = []
    # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
    semaphore = asyncio.Semaphore(20)  # èŠ‚ç‚¹æ£€æµ‹å¹¶å‘æ•°è¾ƒä½ï¼Œé¿å…è¢«å°
    
    async def check_single_node(url):
        async with semaphore:
            return await url_check_valid(url, target, session)
    
    tasks = [check_single_node(url) for url in urls]
    for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=f"æ£€æµ‹{target}èŠ‚ç‚¹"):
        res = await coro
        if res:
            valid_urls.append(res)
    return valid_urls

def write_url_list(url_list, file_path):
    """å°† URL åˆ—è¡¨å†™å…¥æ–‡æœ¬æ–‡ä»¶"""
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write("\n".join(url_list))
    logger.info(f"å·²ä¿å­˜ {len(url_list)} ä¸ªé“¾æ¥åˆ° {file_path}")
from urllib.parse import urlparse

# -------------------------------
# é“¾æ¥å»é‡è¾…åŠ©å‡½æ•° (å¤ç”¨)
# -------------------------------
def get_domain(url):
    """æå– URL çš„ä¸»åŸŸåï¼ˆhostnameï¼‰"""
    try:
        # urlparseè§£æURLï¼Œè·å–ç½‘ç»œä½ç½®ï¼ˆnetlocï¼‰ï¼Œå³åŸŸå+ç«¯å£
        netloc = urlparse(url).netloc
        if not netloc:
            # å¦‚æœnetlocä¸ºç©ºï¼Œå¯èƒ½ä¸æ˜¯ä¸€ä¸ªå®Œæ•´çš„URLï¼Œå°è¯•ç›´æ¥è¿”å›
            return url
        # ç§»é™¤å¯èƒ½çš„ç«¯å£å·ï¼ˆå¦‚:8080ï¼‰
        domain = netloc.split(':')[0]
        # ç§»é™¤ www. å‰ç¼€
        if domain.startswith('www.'):
            domain = domain[4:]
        return domain
    except Exception as e:
        logger.warning(f"æ— æ³•è§£æ URL: {url}ï¼Œå¼‚å¸¸: {e}")
        return url

def deduplicate_urls_by_domain(url_list):
    """
    æ ¹æ®ä¸»åŸŸåå¯¹ URL åˆ—è¡¨è¿›è¡Œå»é‡ã€‚
    ä¿ç•™åˆ—è¡¨ä¸­æ¯ä¸ªä¸»åŸŸåä¸‹çš„ 'æœ€åä¸€ä¸ª' é“¾æ¥ã€‚
    """
    domain_to_url = {}
    
    for url in url_list:
        # å¯¹äº "å¼€å¿ƒç©è€" åˆ—è¡¨ï¼Œé“¾æ¥åœ¨å­—ç¬¦ä¸²çš„æœ«å°¾ï¼Œéœ€è¦å…ˆæå–URL
        cleaned_url = url.split(' ')[-1] if ' ' in url and 'http' in url else url
        
        domain = get_domain(cleaned_url)
        if domain:
            # å­˜å‚¨çš„æ˜¯å®Œæ•´çš„åŸå§‹å­—ç¬¦ä¸²ï¼Œä»¥ä¾¿ä¿ç•™ "å¯ç”¨æµé‡: XX GB" ä¿¡æ¯
            domain_to_url[domain] = url 
        else:
            domain_to_url[url] = url
            
    deduped_urls = list(domain_to_url.values())
    logger.info(f"å»é‡å‰é“¾æ¥æ•°: {len(url_list)}, å»é‡åé“¾æ¥æ•°: {len(deduped_urls)}")
    
    return deduped_urls
# -------------------------------
# ä¸»å‡½æ•°å…¥å£
# -------------------------------
async def validate_existing_subscriptions(config, session):
    """éªŒè¯ç°æœ‰è®¢é˜…çš„æœ‰æ•ˆæ€§ï¼Œç§»é™¤å¤±æ•ˆè®¢é˜…"""
    logger.info("ğŸ” å¼€å§‹éªŒè¯ç°æœ‰è®¢é˜…çš„æœ‰æ•ˆæ€§...")
    
    all_existing_urls = []
    
    # æå–æ‰€æœ‰ç°æœ‰è®¢é˜…URL
    for category in ["æœºåœºè®¢é˜…", "clashè®¢é˜…", "v2è®¢é˜…"]:
        for item in config.get(category, []):
            if isinstance(item, str) and item.strip():
                all_existing_urls.append((item.strip(), category))
    
    # ä»å¼€å¿ƒç©è€ä¸­æå–URL
    for item in config.get("å¼€å¿ƒç©è€", []):
        if isinstance(item, str) and item.strip():
            url_match = re.search(r'https?://[^\s]+', item)
            if url_match:
                all_existing_urls.append((url_match.group(), "å¼€å¿ƒç©è€"))
    
    if not all_existing_urls:
        logger.info("ğŸ“ æ²¡æœ‰ç°æœ‰è®¢é˜…éœ€è¦éªŒè¯")
        return {"æœºåœºè®¢é˜…": [], "clashè®¢é˜…": [], "v2è®¢é˜…": [], "å¼€å¿ƒç©è€": []}
    
    logger.info(f"ğŸ“Š éœ€è¦éªŒè¯ {len(all_existing_urls)} ä¸ªç°æœ‰è®¢é˜…")
    
    # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘
    semaphore = asyncio.Semaphore(30)
    
    async def check_single_existing(url_info):
        url, category = url_info
        async with semaphore:
            result = await sub_check(url, session)
            return (url, category, result)
    
    valid_existing = {"æœºåœºè®¢é˜…": [], "clashè®¢é˜…": [], "v2è®¢é˜…": [], "å¼€å¿ƒç©è€": []}
    tasks = [check_single_existing(url_info) for url_info in all_existing_urls]
    
    for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="éªŒè¯ç°æœ‰è®¢é˜…"):
        url, category, result = await coro
        if result:
            if result["type"] == "æœºåœºè®¢é˜…":
                valid_existing["æœºåœºè®¢é˜…"].append(url)
                if result["info"]:
                    valid_existing["å¼€å¿ƒç©è€"].append(f'{result["info"]} {url}')
            elif result["type"] == "clashè®¢é˜…":
                valid_existing["clashè®¢é˜…"].append(url)
            elif result["type"] == "v2è®¢é˜…":
                valid_existing["v2è®¢é˜…"].append(url)
    
    # ç»Ÿè®¡éªŒè¯ç»“æœ
    total_original = len(all_existing_urls)
    total_valid = sum(len(valid_existing[cat]) for cat in ["æœºåœºè®¢é˜…", "clashè®¢é˜…", "v2è®¢é˜…"])
    
    logger.info(f"âœ… ç°æœ‰è®¢é˜…éªŒè¯å®Œæˆ: {total_original} â†’ {total_valid} (æœ‰æ•ˆç‡: {total_valid/total_original*100:.1f}%)")
    
    return valid_existing

async def main():
    config_path = 'config.yaml'
    
    logger.info("ğŸš€ å¼€å§‹è®¢é˜…ç®¡ç†æµç¨‹...")
    logger.info("=" * 60)
    
    # åŠ è½½ç°æœ‰é…ç½®
    config = load_yaml_config(config_path)
    
    # ç»Ÿè®¡åŸå§‹æ•°æ®
    original_counts = {}
    for category in ["æœºåœºè®¢é˜…", "clashè®¢é˜…", "v2è®¢é˜…", "å¼€å¿ƒç©è€"]:
        original_counts[category] = len(config.get(category, []))
    
    logger.info("ğŸ“Š åŸå§‹é…ç½®ç»Ÿè®¡:")
    for category, count in original_counts.items():
        logger.info(f"   {category}: {count:,} ä¸ª")
    
    # åˆ›å»ºä¼˜åŒ–çš„ä¼šè¯
    connector = aiohttp.TCPConnector(
        limit=100,
        limit_per_host=20,
        ttl_dns_cache=300,
        use_dns_cache=True,
    )
    timeout = aiohttp.ClientTimeout(total=30, connect=10)
    
    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        
        # ç¬¬ä¸€æ­¥ï¼šéªŒè¯ç°æœ‰è®¢é˜…
        logger.info("\nğŸ” ç¬¬ä¸€æ­¥ï¼šéªŒè¯ç°æœ‰è®¢é˜…")
        logger.info("-" * 40)
        valid_existing = await validate_existing_subscriptions(config, session)
        
        # ç¬¬äºŒæ­¥ï¼šè·å–æ–°çš„è®¢é˜…é“¾æ¥
        logger.info("\nğŸ“¡ ç¬¬äºŒæ­¥ï¼šè·å–æ–°çš„è®¢é˜…é“¾æ¥")
        logger.info("-" * 40)
        today_urls = await update_today_sub(session)
        logger.info(f"ğŸ“¥ ä» Telegram é¢‘é“è·å¾— {len(today_urls)} ä¸ªæ–°é“¾æ¥")
        
        # ç¬¬ä¸‰æ­¥ï¼šæ£€æŸ¥æ–°è®¢é˜…çš„æœ‰æ•ˆæ€§
        logger.info("\nğŸ” ç¬¬ä¸‰æ­¥ï¼šæ£€æŸ¥æ–°è®¢é˜…æœ‰æ•ˆæ€§")
        logger.info("-" * 40)
        new_results = await check_subscriptions(today_urls)
        
        # åˆ†ç±»æ–°è®¢é˜…
        new_subs = [res["url"] for res in new_results if res and res["type"] == "æœºåœºè®¢é˜…"]
        new_clash = [res["url"] for res in new_results if res and res["type"] == "clashè®¢é˜…"]
        new_v2 = [res["url"] for res in new_results if res and res["type"] == "v2è®¢é˜…"]
        new_play = [f'{res["info"]} {res["url"]}' for res in new_results 
                   if res and res["type"] == "æœºåœºè®¢é˜…" and res["info"]]
        
        logger.info(f"âœ… æ–°å¢æœ‰æ•ˆè®¢é˜…: æœºåœº{len(new_subs)}ä¸ª, clash{len(new_clash)}ä¸ª, v2{len(new_v2)}ä¸ª")
        
        # ç¬¬å››æ­¥ï¼šåˆå¹¶æœ‰æ•ˆè®¢é˜…
        logger.info("\nğŸ”„ ç¬¬å››æ­¥ï¼šåˆå¹¶æœ‰æ•ˆè®¢é˜…")
        logger.info("-" * 40)
        
        # 1. åˆæ­¥åˆå¹¶å’Œå»é‡ (set() è‡ªåŠ¨å»é‡)
        merged_subs = sorted(list(set(valid_existing["æœºåœºè®¢é˜…"] + new_subs)))
        merged_clash = sorted(list(set(valid_existing["clashè®¢é˜…"] + new_clash)))
        merged_v2 = sorted(list(set(valid_existing["v2è®¢é˜…"] + new_v2)))
        merged_play = sorted(list(set(valid_existing["å¼€å¿ƒç©è€"] + new_play)))
        
        # 2. **æ–°å¢ï¼šä¸»åŸŸåå»é‡**
        logger.info("å¼€å§‹å¯¹ 'æœºåœºè®¢é˜…' åˆ—è¡¨è¿›è¡Œä¸»åŸŸåå»é‡...")
        final_subs_deduped = deduplicate_urls_by_domain(merged_subs)
        
        # 'å¼€å¿ƒç©è€' åŒ…å«æµé‡ä¿¡æ¯ï¼Œä¹Ÿéœ€è¦å»é‡
        logger.info("å¼€å§‹å¯¹ 'å¼€å¿ƒç©è€' åˆ—è¡¨è¿›è¡Œä¸»åŸŸåå»é‡...")
        final_play_deduped = deduplicate_urls_by_domain(merged_play)
        
        # clash å’Œ v2 åœ¨è¿™é‡Œä¸éœ€è¦å»é‡ï¼Œå› ä¸ºå®ƒä»¬ä¼šåœ¨ç¬¬å…­æ­¥ç”Ÿæˆè¾“å‡ºæ–‡ä»¶æ—¶å†æ¬¡å»é‡
        # ä½†ä¸ºäº†ä¿è¯ config.yaml æœ¬èº«æ˜¯å¹²å‡€çš„ï¼Œä¹Ÿè¿›è¡Œå»é‡
        logger.info("å¼€å§‹å¯¹ 'clashè®¢é˜…' åˆ—è¡¨è¿›è¡Œä¸»åŸŸåå»é‡...")
        final_clash_deduped = deduplicate_urls_by_domain(merged_clash)
        
        logger.info("å¼€å§‹å¯¹ 'v2è®¢é˜…' åˆ—è¡¨è¿›è¡Œä¸»åŸŸåå»é‡...")
        final_v2_deduped = deduplicate_urls_by_domain(merged_v2)
        
        final_config = {
            "æœºåœºè®¢é˜…": final_subs_deduped,
            "clashè®¢é˜…": final_clash_deduped,
            "v2è®¢é˜…": final_v2_deduped,
            "å¼€å¿ƒç©è€": final_play_deduped,
            "tgchannel": config.get("tgchannel", [])  # ä¿ç•™é¢‘é“é…ç½®
        }
        
        # ç»Ÿè®¡æœ€ç»ˆç»“æœ
        logger.info("ğŸ“ˆ æœ€ç»ˆç»Ÿè®¡å¯¹æ¯”:")
        total_original = sum(original_counts.values())
        total_final = sum(len(final_config[cat]) for cat in ["æœºåœºè®¢é˜…", "clashè®¢é˜…", "v2è®¢é˜…", "å¼€å¿ƒç©è€"])
        
        for category in ["æœºåœºè®¢é˜…", "clashè®¢é˜…", "v2è®¢é˜…", "å¼€å¿ƒç©è€"]:
            original = original_counts[category]
            final = len(final_config[category])
            change = final - original
            change_str = f"(+{change})" if change > 0 else f"({change})" if change < 0 else "(=)"
            logger.info(f"   {category}: {original:,} â†’ {final:,} {change_str}")
        
        logger.info(f"ğŸ“Š æ€»ä½“: {total_original:,} â†’ {total_final:,} "
                   f"(æ¸…ç†ç‡: {(total_original-total_final)/total_original*100:.1f}%)")
        
        # ä¿å­˜æ›´æ–°åçš„é…ç½®
        save_yaml_config(final_config, config_path)
        logger.info("ğŸ’¾ é…ç½®æ–‡ä»¶å·²æ›´æ–°")
        
        # ç¬¬äº”æ­¥ï¼šç”Ÿæˆè¾“å‡ºæ–‡ä»¶
        logger.info("\nğŸ“ ç¬¬äº”æ­¥ï¼šç”Ÿæˆè¾“å‡ºæ–‡ä»¶")
        logger.info("-" * 40)
        
        # å†™å…¥è®¢é˜…å­˜å‚¨æ–‡ä»¶
        sub_store_file = config_path.replace('.yaml', '_sub_store.txt')
        content = ("-- play_list --\n\n" + 
                  "\n".join(final_config["å¼€å¿ƒç©è€"]) + 
                  "\n\n-- sub_list --\n\n" + 
                  "\n".join(final_config["æœºåœºè®¢é˜…"]))
        with open(sub_store_file, 'w', encoding='utf-8') as f:
            f.write(content)
        logger.info(f"ğŸ“„ è®¢é˜…å­˜å‚¨æ–‡ä»¶å·²ä¿å­˜: {sub_store_file}")
        
        # ç¬¬å…­æ­¥ï¼šæ£€æµ‹èŠ‚ç‚¹æœ‰æ•ˆæ€§
        logger.info("\nğŸ” ç¬¬å…­æ­¥ï¼šæ£€æµ‹èŠ‚ç‚¹æœ‰æ•ˆæ€§")
        logger.info("-" * 40)
        
        # æ£€æµ‹æœºåœºè®¢é˜…èŠ‚ç‚¹
        if final_config["æœºåœºè®¢é˜…"]:
            valid_loon = await check_nodes(final_config["æœºåœºè®¢é˜…"], "loon", session)
            
            # --- æ–°å¢å»é‡é€»è¾‘ ---
            if valid_loon:
                logger.info("å¼€å§‹å¯¹ loon è®¢é˜…é“¾æ¥è¿›è¡Œä¸»åŸŸåå»é‡...")
                valid_loon = deduplicate_urls_by_domain(valid_loon)
            # --------------------
            
            loon_file = config_path.replace('.yaml', '_loon.txt')
            write_url_list(valid_loon, loon_file)
        
        # æ£€æµ‹clashè®¢é˜…èŠ‚ç‚¹
        if final_config["clashè®¢é˜…"]:
            valid_clash = await check_nodes(final_config["clashè®¢é˜…"], "clash", session)
            
            # --- æ–°å¢å»é‡é€»è¾‘ ---
            if valid_clash:
                logger.info("å¼€å§‹å¯¹ clash è®¢é˜…é“¾æ¥è¿›è¡Œä¸»åŸŸåå»é‡...")
                valid_clash = deduplicate_urls_by_domain(valid_clash)
            # --------------------
            
            clash_file = config_path.replace('.yaml', '_clash.txt')
            write_url_list(valid_clash, clash_file)
        
        # æ£€æµ‹v2è®¢é˜…èŠ‚ç‚¹
        if final_config["v2è®¢é˜…"]:
            valid_v2 = await check_nodes(final_config["v2è®¢é˜…"], "v2ray", session)
            
            # --- æ–°å¢å»é‡é€»è¾‘ ---
            if valid_v2:
                logger.info("å¼€å§‹å¯¹ v2 è®¢é˜…é“¾æ¥è¿›è¡Œä¸»åŸŸåå»é‡...")
                valid_v2 = deduplicate_urls_by_domain(valid_v2)
            # --------------------
            
            v2_file = config_path.replace('.yaml', '_v2.txt')
            write_url_list(valid_v2, v2_file)
    
    logger.info("\nğŸ‰ è®¢é˜…ç®¡ç†æµç¨‹å®Œæˆï¼")
    logger.info("=" * 60)

if __name__ == '__main__':
    asyncio.run(main())
